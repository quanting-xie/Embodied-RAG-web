<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html>

<!-- ======================================================================= -->
<script src="http://www.google.com/jsapi" type="text/javascript"></script>
<script type="text/javascript">google.load("jquery", "1.3.2");</script>
<script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.4.1/js/bootstrap.min.js"></script>
<style type="text/css"> 
  body {
    font-family: "Titillium Web","HelveticaNeue-Light", "Helvetica Neue Light", "Helvetica Neue", Helvetica, Arial, "Lucida Grande", sans-serif;
    font-weight:300;
    font-size:18px;
    margin-left: auto;
    margin-right: auto;
    width: 1100px;
  }

  h1 {
    font-weight:300;
  }

  .disclaimerbox {
    background-color: #eee;
    border: 1px solid #eeeeee;
    border-radius: 10px ;
    -moz-border-radius: 10px ;
    -webkit-border-radius: 10px ;
    padding: 20px;
  }

  video.header-vid {
    height: 140px;
    border: 1px solid black;
    border-radius: 10px ;
    -moz-border-radius: 10px ;
    -webkit-border-radius: 10px ;
  }

  img.header-img {
    height: 140px;
    border: 1px solid black;
    border-radius: 10px ;
    -moz-border-radius: 10px ;
    -webkit-border-radius: 10px ;
  }

  img.rounded {
    border: 1px solid #eeeeee;
    border-radius: 10px ;
    -moz-border-radius: 10px ;
    -webkit-border-radius: 10px ;
  }

  .link_preview {
		border: 0px solid #ddd; /* Gray border */
		border-radius: 4px;  /* Rounded border */
		padding: 5px; /* Some padding */
	 /* Set a small width */
	}

  .link_preview:hover {
		box-shadow: 0 0 2px 1px rgba(0, 140, 186, 0.5);
	}

  a:link,a:visited
  {
    color: #1367a7;
    text-decoration: none;
  }
  a:hover {
    color: #208799;
  }

  td.dl-link {
    height: 160px;
    text-align: center;
    font-size: 22px;
  }

  .layered-paper-big { /* modified from: http://css-tricks.com/snippets/css/layered-paper/ */
    box-shadow:
            0px 0px 1px 1px rgba(0,0,0,0.35), /* The top layer shadow */
            5px 5px 0 0px #fff, /* The second layer */
            5px 5px 1px 1px rgba(0,0,0,0.35), /* The second layer shadow */
            10px 10px 0 0px #fff, /* The third layer */
            10px 10px 1px 1px rgba(0,0,0,0.35), /* The third layer shadow */
            15px 15px 0 0px #fff, /* The fourth layer */
            15px 15px 1px 1px rgba(0,0,0,0.35), /* The fourth layer shadow */
            20px 20px 0 0px #fff, /* The fifth layer */
            20px 20px 1px 1px rgba(0,0,0,0.35), /* The fifth layer shadow */
            25px 25px 0 0px #fff, /* The fifth layer */
            25px 25px 1px 1px rgba(0,0,0,0.35); /* The fifth layer shadow */
    margin-left: 10px;
    margin-right: 45px;
  }


  .layered-paper { /* modified from: http://css-tricks.com/snippets/css/layered-paper/ */
    box-shadow:
            0px 0px 1px 1px rgba(0,0,0,0.35), /* The top layer shadow */
            5px 5px 0 0px #fff, /* The second layer */
            5px 5px 1px 1px rgba(0,0,0,0.35), /* The second layer shadow */
            10px 10px 0 0px #fff, /* The third layer */
            10px 10px 1px 1px rgba(0,0,0,0.35); /* The third layer shadow */
    margin-top: 5px;
    margin-left: 10px;
    margin-right: 30px;
    margin-bottom: 5px;
  }

  .vert-cent {
    position: relative;
      top: 50%;
      transform: translateY(-50%);
  }

  hr
  {
    border: 0;
    height: 1px;
    background-image: linear-gradient(to right, rgba(0, 0, 0, 0), rgba(0, 0, 0, 0.75), rgba(0, 0, 0, 0));
  }

  #authors td {
    padding-bottom:5px;
    padding-top:30px;
  }
</style>
<!-- ======================================================================= -->
    
<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-114291442-3"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-114291442-3');
</script>



<script type="text/javascript" src="resources/hidebib.js"></script>
<link href='https://fonts.googleapis.com/css?family=Titillium+Web:400,600,400italic,600italic,300,300italic' rel='stylesheet' type='text/css'>

<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <link rel="icon" type="image/png" href="../img/favicon.ico">
  <title>Embodied-RAG: General Non-parametric Embodied
    Memory for Retrieval and Generation</title>
  <meta name="HandheldFriendly" content="True" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <script src="https://www.youtube.com/iframe_api"></script>
  
</head>

<body>
      <br><br><br>
      <center><span style="font-size:50px;font-weight:bold;width:550px">Embodied-RAG: General Non-parametric Embodied
        Memory for Retrieval and Generation</span></center><br/>
      <table align=center width=800px>
      <tr>
        <td align=center width=200px>
          <center><span style="font-size:22px"><a href="https://quantingxie.github.io/" target="_blank">Quanting Xie *</a></span></center></td>
          
        <td align=center width=200px>
        <center><span style="font-size:22px"><a href="https://soyeonm.github.io/" target="_blank">So Yeon Min *</a></span></center></td>

        <td align=center width=200px>
        <center><span style="font-size:22px"><a href="https://tyz1030.github.io/" target="_blank">Tianyi Zhang</span></center></td>
        </tr>

        <tr>
        </tr>
      </table>

    <table align=center width=1000px>
        <tr>
          <td align=center width=50px>
        <center><span style="font-size:22px"><a href="https://www.linkedin.com/in/aarav-bajaj-408ab01b1/" target="_blank">Aarav Bajaj</a></span></center></td>
        <td align=center width=50px>
        <center><span style="font-size:22px"><a href="https://www.ri.cmu.edu/ri-faculty/matt-johnson-roberson/" target="_blank">Matthew Johnson-Roberson</a></span></center></td>
      
        <td align=center width=50px>
        <center><span style="font-size:22px"><a href="https://yonatanbisk.com" target="_blank">Yonatan Bisk</a></span></center></td>
        
        
    

        </tr>
      <tr>
      </tr>
      </table>
    
      
    <!-- <table align=center width=700px>
	  <tr>
          <tr/>
          <tr>
            <td align=center width=700px><center><span style="font-size:22px">Currently the leading entry of <a href="https://leaderboard.allenai.org/alfred/submissions/public"> ALFRED leaderboard</a> </span></center></td>
          <tr/>
    </table><br/> -->
    

    <table align=center width=700px>
          <tr>
            <td align=center width=700px><center><span style="font-size:2px">Carnegie Mellon University</a> </span></center></td>
          <tr/>
    </table><br/>
    
    
      <table align=center width=700px>
          <tr>
            <!-- <td align=center width=100px><center><span style="font-size:28px"><a href="https://arxiv.org/abs/2212.05923">[Paper]</a></span></center></td> -->
             <td align=center width=100px><center><span style="font-size:28px"><a href="https://arxiv.org/abs/2407.12061">[Paper]</a></span></center></td> 
            <td align=center width=100px><center><span style="font-size:28px"><a href="https://github.com/soyeonm/SIF_github">[Code and Data]</a></span></center></td>
            <!-- <td align=center width=100px><center><span style="font-size:28px">[Code] (will be released soon)</a></span></center></td> -->
<!--             <td align=center width=100px><center><span style="font-size:28px"><a href="https://youtu.be/4pk_5_NEW_s">[Talk]</a></span></center></td>
            <td align=center width=100px><center><span style="font-size:28px"><a href="../talks/cvpr20-semantic-exploration.pdf">[Slides]</a></span></center></td> -->
<!--             <td align=center width=100px><center><span style="font-size:28px"><a href='https://github.com/devendrachaplot/Object-Goal-Navigation'>[Code]</a></span></center></td>
 -->          <tr/>
      </table><br/>

       
      <center><h1>Video Presentation</h1></center>

      <table align="center" width="1000px">
        <tr>
          <td align="center" width="1000px">
            <iframe width="800" height="450" src="https://www.youtube.com/embed/LcB89Rdyxhg" title="Demo1" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
          </td>
        </tr>
      </table>
      <br>

      <br><hr>

      <center><h1>What is needed to construct non-parametric memory for embodied agent?</h1></center>
<!--       <center><h2>Project Video</h2></center> -->
      <table align=center width=300px>
      <tr><td align=center width=300px>
          <!-- <iframe width="768" height="432" src="https://www.youtube.com/embed/tlyz68j_jvE" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
          <center><img src = "./resources/tests_unseen_33.gif" height="400px"></img></a><br></center> -->
          <center><img src = "./resources/Figure_1_overview.png" height="500px"></img></a><br></center>
      </td></tr>
      </table>
      <br>

      <div style="width:800px; margin:0 auto; text-align:justify"> 
        There is no limit to how much a robot might explore and learn, but all of that knowledge needs to be searchable and actionable. Within language research, retrieval augmented generation (RAG) has become the workhouse of large-scale non-parameteric knowledge, however existing techniques do not directly transfer to the embodied domain, which is multimodal, data is highly correlated, and perception requires abstraction.

        To address these challenges, we introduce Embodied-RAG, a framework that enhances the foundational model of an embodied agent with a non-parametric memory system capable of autonomously constructing hierarchical knowledge for both navigation and language generation. Embodied-RAG handles a full range of spatial and semantic resolutions across diverse environments and query types, whether for a specific object or a holistic description of ambiance. 
        At its core, Embodied-RAG's memory is structured as a semantic forest, storing language descriptions at varying levels of detail. This hierarchical organization allows the system to efficiently generate context-sensitive outputs across different robotic platforms. We demonstrate that Embodied-RAG effectively bridges RAG to the robotics domain, successfully handling over 200 explanation and navigation queries across 19 environments, highlighting its promise for general-purpose non-parametric system for embodied agents.

      <br><hr>

      <center><h1>Task</h1></center>

      <table align="center" width="300px">
      <tr><td align="center" width="300px">
          <center><img src="./resources/Task_figure.png" height="300px"></center>
          <center><img src="./resources/Task Table.png" height="300px"></center>

      </td></tr>
      </table>
      
      <!-- Replacing white text with your provided content -->
      <div style="text-align: justify;">
          <p>The Embodied-RAG benchmark contains queries from the cross-product of <strong>{explicit, implicit, global}</strong> questions with potential <strong>{navigational action, language}</strong> generation outputs.</p>
      
          <h3>A task consists of:</h3>
          <ul>
              <li><strong>Query</strong>: The content can be explicit (e.g., a particular object instance), implicit (e.g., adequacy, instructions requiring pragmatic understanding), or global. The request may pertain to a specific location or overall ambiance.</li>
              <li><strong>Experience</strong>: The experience is a sequence of egocentric visual perception and odometry, occurring in indoor, outdoor, or mixed environments.</li>
              <li><strong>Output</strong>: The expected output may include both navigation actions with language descriptions (Fig. 1 top) or language explanations (Fig. 1 bottom).</li>
          </ul>
      
          <p>Example tasks are shown in Fig. 1, with instances of explicit, implicit, and global queries. Spatially, the queries range from specific regions small enough to contain certain objects to global regions encompassing the entire scene. Linguistically, global queries are closer to retrieval-augmented generation tasks, while explicit/implicit ones are more retrieval-focused. Explicit and implicit queries are navigational tasks that expect navigation actions and text descriptions of the retrieved location. Global queries, on the other hand, are explanation tasks requiring text generation at a more holistic level. There are no global navigation tasks, as they pertain to larger areas, sometimes the entire environment.</p>

      </div>
      
      <br><hr>

      <center><h1>Method</h1></center>

      <table align=center width=300px>
      <tr><td align=center width=300px>

          <center><img src = "./resources/Figure2_method_.png" height="350px"></img></a><br></center>
      </td></tr>
      </table>

      <p>We propose <strong>Embodied-RAG</strong>, which integrates a topological map and a semantic forest for retrieval-augmented navigation and reasoning. The method operates in three stages:</p>

      <ul>
        <li><strong>Memory Construction</strong>: A topological graph is built from nodes containing allocentric coordinates, yaw, ego-centric image paths, and captions generated by a vision-language model. These nodes form a memory-efficient structure for large-scale environments. A hierarchical semantic forest is created by clustering these nodes, with non-leaf nodes summarizing their child nodes using an LLM.</li>
        
        <li><strong>Retrieval</strong>: We retrieve the top $k$ chains of semantic information by traversing the forest with BFS and LLM-guided selection, refining the query relevance at each level. This approach extracts multi-scale contextual paths that contain both semantic and spatial information.</li>
        
        <li><strong>Generation</strong>: The retrieved chains are used as context for generating either navigational actions or text-based responses. The LLM selects the optimal waypoint for navigation, or generates detailed answers based on the query, leveraging the hierarchical structure to address explicit, implicit, and global queries.</li>
      </ul>
      
      <hr>

      <center><h1>Results</h1></center>

      <table align="center" width="300px">
        <tr>
          <td align="center" width="300px">
      
            <!-- Result Table with caption -->
            <figure>
              <center>
                <img src="./resources/Result_table.png" height="100px" alt="Result Table"></img>
                <figcaption><strong>Result Table</strong>: Comparison of methods across different tasks.</figcaption>
              </center>
            </figure>
      
            <!-- Result Figure with caption -->
            <figure>
              <center>
                <img src="./resources/result.png" height="600px" alt="Result Figure"></img>
                <figcaption><strong>Result Figure</strong>: Example of reasoning output for generation tasks.</figcaption>
              </center>
            </figure>
      
          </td>
        </tr>
      </table>
      
      <h3>Quantitative Results</h3>
      <p>Table <strong>Result Table</strong> presents the performance of Embodied-RAG compared to RAG and Semantic Match across explicit, implicit, and global retrieval tasks. Embodied-RAG consistently outperforms the baselines across both small and large environments. Explicit queries show strong performance across all methods, with Embodied-RAG slightly improving over RAG. For implicit queries, Embodied-RAG maintains high success rates, while RAG and Semantic Match experience significant drops, particularly in larger environments. In global queries, Embodied-RAG achieves the highest scores on the Likert scale, while Semantic Match is inapplicable due to its inability to summarize or reason globally.</p>
      
      <h3>Qualitative Results</h3>
      <p>In qualitative comparisons, Embodied-RAG demonstrates superior reasoning capabilities, particularly for implicit and global queries (see <strong>Result Figure</strong>). For the query <em>“Find where I can buy some drinks?”</em>, Embodied-RAG accurately identifies food service areas, while the baselines retrieve less appropriate results like refrigerators or water fountains. Similarly, for the query <em>“Find somewhere to take a nap outside”</em>, Embodied-RAG identifies public parks, while RAG and Semantic Match incorrectly suggest private backyards, lacking global context.</p>
      
      <br><hr>

      <center><h1>More Demos</h1></center>

      <table align="center" width="900px" style="border-collapse: separate; border-spacing: 20px 20px;">
      
        <!-- First Row of Videos -->
        <tr>
          <td align="center" width="500px">
            <iframe width="450" height="250" src="https://www.youtube.com/embed/WEjQX-t-dWI" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
            <br>
            <input type="text" placeholder="Premapping outdoor-indoor multifunctional environment" style="width:450px; text-align:center; font-size:16px; font-weight:bold; margin-top:10px; padding:5px;">
          </td>
          <td align="center" width="500px">
            <iframe width="450" height="250" src="https://www.youtube.com/embed/d1aw2orzOc8" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
            <br>
            <input type="text" placeholder="Premapping indoor environment" style="width:450px; text-align:center; font-size:16px; font-weight:bold; margin-top:10px; padding:5px;">
          </td>
        </tr>
              
      </table>

      <table align="center" width="900px" style="border-collapse: separate; border-spacing: 10px 20px;">

        <!-- Second Row of Videos -->
        <tr>
          <td align="center">
            <iframe width="300" height="200" src="https://www.youtube.com/embed/WNwDiAca5bg" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
            <br>
            <input type="text" placeholder="Implicit Query Demo 2" style="width:300px; text-align:center; font-size:16px; font-weight:bold; margin-top:10px; padding:5px;">
          </td>
          <td align="center">
            <iframe width="300" height="200" src="https://www.youtube.com/embed/d1aw2orzOc8" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
            <br>
            <input type="text" placeholder="Explicit Query" style="width:300px; text-align:center; font-size:16px; font-weight:bold; margin-top:10px; padding:5px;">
          </td>
          <td align="center">
            <iframe width="300" height="200" src="https://www.youtube.com/embed/NEW_VIDEO_ID" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
            <br>
            <input type="text" placeholder="Description for third video" style="width:300px; text-align:center; font-size:16px; font-weight:bold; margin-top:10px; padding:5px;">
          </td>
        </tr>
      
      </table>

      <center><h1>Example Queries and Prompt</h1></center>

      <h2>Embodied Generation Task Examples</h2>
      
      <h3>Queries</h3>
      
      <h4>Explicit Retrieval Task</h4>
      <pre>
      1. Find me a water fountain.
      2. Find me a sofa.
      3. Find me a fire hydrant.
      </pre>
      
      <h4>Implicit Retrieval Task</h4>
      <pre>
      1. I am dehydrated, find me somewhere.
      2. Find me a quiet place to read books.
      3. Find me a place suitable for having a group discussion.
      4. Find me a place suitable for camping.
      5. Find me somewhere I can play with my children but not on the grass.
      </pre>
      
      <h4>Global Retrieval Task</h4>
      <pre>
      1. Can you describe the overall atmosphere of this environment?
      2. How are the safety features in this environment?
      3. Is this environment prepared for a fire hazard?
      4. Can you describe the overall plant trends in this environment?
      5. Can you describe different areas in this environment?
      </pre>
      
      <h2>Embodied-RAG Prompts</h2>
      
      <h3>Memory</h3>
      
      <h4>Caption Prompt</h4>
      <blockquote style="border-left: 5px solid #ccc; padding-left: 10px;">
      You are a robot equipped with three front cameras. Given three images, describe the objects you see in a single list, and then describe their spatial relationships.
      </blockquote>
      
      <h4>Abstraction Prompt</h4>
      <blockquote style="border-left: 5px solid #ccc; padding-left: 10px;">
      Given the environment descriptions {environment descriptions}, can you abstract them into a more general form? Try to infer the environment's intrinsic properties.
      </blockquote>
      
      <h3>Retrieval</h3>
      
      <h4>Selection Prompt</h4>
      <blockquote style="border-left: 5px solid #ccc; padding-left: 10px;">
      Given the environment descriptions: {environment descriptions}, select the best one to satisfy the query: {query} and show your reasoning. Structure your answer like this: Reasoning: &lt;reasoning&gt; , Node: &lt;node_1&gt;.
      </blockquote>
      
      <h3>Generation</h3>
      
      <h4>Global Answer Generation</h4>
      <blockquote style="border-left: 5px solid #ccc; padding-left: 10px;">
      Utilize all available environment descriptions: {environment descriptions}, to provide a comprehensive answer to the question: {query}.
      </blockquote>
      
      <hr>


      <center><h1>Environments</h1></center>

      <center><h3>19X diverse environments,  6X Real, 13X sim in Airsim and Habitat Sim</h3></center>
      <center><img src = "./resources/environments.png" height="350px"></img></a><br></center>



      <!-- <center><h1>Video Presentation</h1></center>

      <table align=center width=1000px>
      <tr><td align=center width=1000px>
          <iframe width="800" height="450" src="https://www.youtube.com/embed/iYQqetnCkJM" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
      </td></tr>
      </table>
      <br>
 -->
      

      <!-- <table align=center width=1000px>
        <p style="margin-top:4px;"></p>
        <tr><td width=1200px>
          <center><img src = "./resources/Teaser8.png" width="800px"></img><br></center>
        </td></tr>
      </table>
      <table align=center width=1000px>
        <p style="margin-top:4px;"></p>
        <tr><td width=1200px>
          <center><img src = "./resources/overview6.png" width="800px"></img><br></center>
        </td></tr>
      </table>
      <br/><hr> -->

      <!-- <center><h1>Short Presentation</h1></center>
      <div style="width:800px; margin:0 auto; text-align:justify">
        A short presentation at the CVPR 2020 Embodied AI Workshop describing our winning entry to the Habitat ObjectNav Challenge.
      </div><br/>
      <table align=center width=300px>
      <tr><td align=center width=300px>
        <iframe width="800" height="450" src="https://www.youtube.com/embed/4pk_5_NEW_s" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
      </td></tr>
      </table>
      <br><hr>

      <center><h1>Demo Video</h1></center>
      <table align=center width=300px>
      <tr><td align=center width=300px>
        <iframe width="640" height="360" src="https://www.youtube.com/embed/h56dA2uxpGU" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
      </td></tr>
      </table>
      <br><hr>

      <center><h1>Real-World Transfer</h1></center>
      <table align=center width=1000px>
        <p style="margin-top:4px;"></p>
        <tr><td width=1200px>
          <center><img src = "resources/semexp_real_world.gif" width="640px"></img><br></center>
        </td></tr>
      </table>
      <br/><hr>

	<center><h1>Media</h1></center>
	<div style="width:800px; margin:0 auto; text-align:justify">
        A short video created by CMU Media team describing the key ideas behind this project.   
     	 </div>
      <table align=center width=300px>
      <tr><td align=center width=300px>
        <iframe width="800" height="450" src="https://www.youtube.com/embed/FhIut4bqFyw" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
      </td></tr>
      </table>

      <br/>
 	
	<div style="width:800px; margin:0 auto; text-align:justify">
        Read more about the project in the following media articles:   
     	 </div>
	<table align=center width=700px>
	          <tr>
            <td align=left width=120px><span style="font-size:28px"><a onclick="window.open(this.href); return false;" href="https://techcrunch.com/2020/07/20/cmu-and-facebook-ai-research-use-machine-learning-to-teach-robots-to-navigate-by-recognizing-objects/" onkeypress="window.open(this.href); return false;">
									<img src="logos/tc.png" class="link_preview" 	style="max-width:130px;">
									</a></span></td>
            <td align=center width=120px><center><span style="font-size:28px"><a onclick="window.open(this.href); return false;" href="https://gizmodo.com/teaching-robots-there-are-no-toilets-in-the-kitchen-mak-1844446396" onkeypress="window.open(this.href); return false;">
									<img src="logos/gizmodo.png" class="link_preview" style="max-width:150px;">
									</a></span></center></td>
            <td align=center width=120px><center><span style="font-size:28px"><a onclick="window.open(this.href); return false;" href="https://www.thehindu.com/sci-tech/technology/a-navigation-system-powered-by-machine-learning-is-training-robots-to-recognise-objects/article32218732.ece" onkeypress="window.open(this.href); return false;">
									<img src="logos/hindu.png" class="link_preview" style="max-width:150px;">
									</a></span></center></td>
	    <td align=center width=120px><center><span style="font-size:28px"><a onclick="window.open(this.href); return false;" href="https://www.cmu.edu/news/stories/archives/2020/july/robot-navigation.html" onkeypress="window.open(this.href); return false;">
									<img src="logos/cmu.png" class="link_preview" style="max-width:150px;">
						</a></span></center></td>
          <tr/>
      </table>
      <br/><hr> -->

      




      <table align=center width=850px>
        <center><h1>Paper and Bibtex</h1></center>
        <tr>
        <td width=200px align=left>
        <!-- <p style="margin-top:4px;"></p> -->
        <a href="https://arxiv.org/abs/2212.05923"><img style="width:200px" src="resources/paper-cover.png"/></a>
        <center>
        <span style="font-size:20pt"><a href="https://arxiv.org/abs/2407.12061">[Paper]</a>
<!--        <span style="font-size:20pt"><a href="https://arxiv.org/abs/1902.05546v2">[ArXiv]</a>-->
<!--        <span style="font-size:20pt"><a href="resources/slides.pdf">[Slides]</a></span>-->
<!--        <span style="font-size:20pt"><a href="resources/poster.pdf">[Poster]</a></span>-->
        </center>
        </td>
        <td width=50px align=center>
        </td>
        <td width=550px align=left>
        <!-- <p style="margin-top:4px;"></p> -->
<!--            Chaplot, D.S., Gandhi, D., Gupta, S., Gupta, A. and Salakhutdinov, R., 2020. Learning To Explore Using Active Neural SLAM. In International Conference on Learning Representations (ICLR).-->
        <p style="text-align:left;"><b><span style="font-size:20pt">Citation</span></b><br/><span style="font-size:6px;">&nbsp;<br/></span> <span style="font-size:15pt">Xie,Q., Min,S., Zhang,T., Johnson-Roberson, M., Bisk, Y. <br>
          Embodied-RAG: General Non-parametric Embodied
          Memory for Retrieval and Generation.
            <!-- <br> <em> ArXiv, abs/2212.05923.</em> </span></p> -->
        <!-- <p style="margin-top:20px;"></p> -->
        <!-- <span style="font-size:20pt"><a shape="rect" href="javascript:togglebib('assemblies19_bib')" class="togglebib">[Bibtex]</a></span> -->
        <div class="paper" id="assemblies19_bib">
                <pre xml:space="preserve">
Bibtex coming soon!
<!-- @inproceedings{min2024situatedinstructionfollowing,
      title={Situated Instruction Following},
      author={So Yeon Min and Xavi Puig and Devendra Singh Chaplot and Tsung-Yen Yang 
      and Akshara Rai and Priyam Parashar and Ruslan Salakhutdinov and Yonatan Bisk 
      and Roozbeh Mottaghi},
      booktitle={ECCV},
      year={2024}, -->
<!-- }           </pre> -->
          </div>
        </td>
        </tr>
        <tr>
        <td width=250px align=left>
        </td>
        <td width=50px align=center>
        </td>
        <td width=550px align=left>
          
          </td>
          </tr>
      </table>
    <br><hr>
  <br><br>
<script xml:space="preserve" language="JavaScript">
hideallbibs();
</script>
</body>
</html>

