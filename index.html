<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html>

<!-- ======================================================================= -->
<script src="http://www.google.com/jsapi" type="text/javascript"></script>
<script type="text/javascript">google.load("jquery", "1.3.2");</script>
<script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.4.1/js/bootstrap.min.js"></script>
<style type="text/css"> 
  body {
    font-family: "Titillium Web","HelveticaNeue-Light", "Helvetica Neue Light", "Helvetica Neue", Helvetica, Arial, "Lucida Grande", sans-serif;
    font-weight:300;
    font-size:18px;
    margin-left: auto;
    margin-right: auto;
    width: 1100px;
  }

  h1 {
    font-weight:300;
  }

  .disclaimerbox {
    background-color: #eee;
    border: 1px solid #eeeeee;
    border-radius: 10px ;
    -moz-border-radius: 10px ;
    -webkit-border-radius: 10px ;
    padding: 20px;
  }

  video.header-vid {
    height: 140px;
    border: 1px solid black;
    border-radius: 10px ;
    -moz-border-radius: 10px ;
    -webkit-border-radius: 10px ;
  }

  img.header-img {
    height: 140px;
    border: 1px solid black;
    border-radius: 10px ;
    -moz-border-radius: 10px ;
    -webkit-border-radius: 10px ;
  }

  img.rounded {
    border: 1px solid #eeeeee;
    border-radius: 10px ;
    -moz-border-radius: 10px ;
    -webkit-border-radius: 10px ;
  }

  .link_preview {
		border: 0px solid #ddd; /* Gray border */
		border-radius: 4px;  /* Rounded border */
		padding: 5px; /* Some padding */
	 /* Set a small width */
	}

  .link_preview:hover {
		box-shadow: 0 0 2px 1px rgba(0, 140, 186, 0.5);
	}

  a:link,a:visited
  {
    color: #1367a7;
    text-decoration: none;
  }
  a:hover {
    color: #208799;
  }

  td.dl-link {
    height: 160px;
    text-align: center;
    font-size: 22px;
  }

  .layered-paper-big { /* modified from: http://css-tricks.com/snippets/css/layered-paper/ */
    box-shadow:
            0px 0px 1px 1px rgba(0,0,0,0.35), /* The top layer shadow */
            5px 5px 0 0px #fff, /* The second layer */
            5px 5px 1px 1px rgba(0,0,0,0.35), /* The second layer shadow */
            10px 10px 0 0px #fff, /* The third layer */
            10px 10px 1px 1px rgba(0,0,0,0.35), /* The third layer shadow */
            15px 15px 0 0px #fff, /* The fourth layer */
            15px 15px 1px 1px rgba(0,0,0,0.35), /* The fourth layer shadow */
            20px 20px 0 0px #fff, /* The fifth layer */
            20px 20px 1px 1px rgba(0,0,0,0.35), /* The fifth layer shadow */
            25px 25px 0 0px #fff, /* The fifth layer */
            25px 25px 1px 1px rgba(0,0,0,0.35); /* The fifth layer shadow */
    margin-left: 10px;
    margin-right: 45px;
  }


  .layered-paper { /* modified from: http://css-tricks.com/snippets/css/layered-paper/ */
    box-shadow:
            0px 0px 1px 1px rgba(0,0,0,0.35), /* The top layer shadow */
            5px 5px 0 0px #fff, /* The second layer */
            5px 5px 1px 1px rgba(0,0,0,0.35), /* The second layer shadow */
            10px 10px 0 0px #fff, /* The third layer */
            10px 10px 1px 1px rgba(0,0,0,0.35); /* The third layer shadow */
    margin-top: 5px;
    margin-left: 10px;
    margin-right: 30px;
    margin-bottom: 5px;
  }

  .vert-cent {
    position: relative;
      top: 50%;
      transform: translateY(-50%);
  }

  hr
  {
    border: 0;
    height: 1px;
    background-image: linear-gradient(to right, rgba(0, 0, 0, 0), rgba(0, 0, 0, 0.75), rgba(0, 0, 0, 0));
  }

  #authors td {
    padding-bottom:5px;
    padding-top:30px;
  }
</style>
<!-- ======================================================================= -->
    
<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-114291442-3"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-114291442-3');
</script>



<script type="text/javascript" src="resources/hidebib.js"></script>
<link href='https://fonts.googleapis.com/css?family=Titillium+Web:400,600,400italic,600italic,300,300italic' rel='stylesheet' type='text/css'>

<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <link rel="icon" type="image/png" href="../img/favicon.ico">
  <title>Embodied-RAG: General Nonparametric Embodied
    Memory for Retrieval and Generation</title>
  <meta name="HandheldFriendly" content="True" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <script src="https://www.youtube.com/iframe_api"></script>
  
</head>

<body>
      <br>
      <center><span style="font-size:50px;font-weight:bold;width:600px">Embodied-RAG: General Nonparametric Embodied
        Memory for Retrieval and Generation</span></center><br/>
      <table align=center width=800px>
      <tr>
        <td align=center width=200px>
          <center><span style="font-size:22px"><a href="https://quantingxie.github.io/" target="_blank">Quanting Xie *</a></span></center></td>
          
        <td align=center width=200px>
        <center><span style="font-size:22px"><a href="https://soyeonm.github.io/" target="_blank">So Yeon Min *</a></span></center></td>

        <td align=center width=200px>
        <center><span style="font-size:22px"><a href="https://tyz1030.github.io/" target="_blank">Tianyi Zhang</span></center></td>
        </tr>

        <tr>
        </tr>
      </table>

    <table align=center width=1000px>
        <tr>
          <td align=center width=50px>
        <center><span style="font-size:22px"><a href="https://www.linkedin.com/in/aarav-bajaj-408ab01b1/" target="_blank">Aarav Bajaj</a></span></center></td>
        <td align=center width=50px>
        <center><span style="font-size:22px"><a href="https://www.ri.cmu.edu/ri-faculty/matt-johnson-roberson/" target="_blank">Matthew Johnson-Roberson</a></span></center></td>
      
        <td align=center width=50px>
        <center><span style="font-size:22px"><a href="https://yonatanbisk.com" target="_blank">Yonatan Bisk</a></span></center></td>
        
        
    

        </tr>
      <tr>
      </tr>
      </table>
    
      
    <!-- <table align=center width=700px>
	  <tr>
          <tr/>
          <tr>
            <td align=center width=700px><center><span style="font-size:22px">Currently the leading entry of <a href="https://leaderboard.allenai.org/alfred/submissions/public"> ALFRED leaderboard</a> </span></center></td>
          <tr/>
    </table><br/> -->
    

    <table align=center width=700px>
          <tr>
            <td align=center width=700px><center><span style="font-size:2px">Carnegie Mellon University</a> </span></center></td>
          <tr/>
    </table><br/>
    
    
      <table align=center width=700px>
          <tr>
            <!-- <td align=center width=100px><center><span style="font-size:28px"><a href="https://arxiv.org/abs/2212.05923">[Paper]</a></span></center></td> -->
             <td align=center width=100px><center><span style="font-size:28px"><a href="https://arxiv.org/abs/2407.12061">[Paper]</a></span></center></td> 
            <td align=center width=100px><center><span style="font-size:28px"><a href="https://github.com/soyeonm/SIF_github">[Code and Data]</a></span></center></td>
            <!-- <td align=center width=100px><center><span style="font-size:28px">[Code] (will be released soon)</a></span></center></td> -->
<!--             <td align=center width=100px><center><span style="font-size:28px"><a href="https://youtu.be/4pk_5_NEW_s">[Talk]</a></span></center></td>
            <td align=center width=100px><center><span style="font-size:28px"><a href="../talks/cvpr20-semantic-exploration.pdf">[Slides]</a></span></center></td> -->
<!--             <td align=center width=100px><center><span style="font-size:28px"><a href='https://github.com/devendrachaplot/Object-Goal-Navigation'>[Code]</a></span></center></td>
 -->          <tr/>
      </table><br/>

       
      <br><hr>

      <center><h1>What is needed to construct non-parametric memory for embodied agent?</h1></center>
<!--       <center><h2>Project Video</h2></center> -->
      <table align=center width=300px>
      <tr><td align=center width=300px>
          <!-- <iframe width="768" height="432" src="https://www.youtube.com/embed/tlyz68j_jvE" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
          <center><img src = "./resources/tests_unseen_33.gif" height="400px"></img></a><br></center> -->
          <center><img src = "./resources/sif_motivation.png" height="300px"></img></a><br></center>
      </td></tr>
      </table>
      <br>

      <div style="width:800px; margin:0 auto; text-align:justify"> 
        There is no limit to how much a robot might explore and learn, but all of that knowledge needs to be searchable and actionable. Within language research, retrieval augmented generation (RAG) has become the workhouse of large-scale non-parameteric knowledge, however existing techniques do not directly transfer to the embodied domain, which is multimodal, data is highly correlated, and perception requires abstraction.

        To address these challenges, we introduce Embodied-RAG, a framework that enhances the foundational model of an embodied agent with a nonparametric memory system capable of autonomously constructing hierarchical knowledge for both navigation and language generation. Embodied-RAG handles a full range of spatial and semantic resolutions across diverse environments and query types, whether for a specific object or a holistic description of ambiance. 
        At its core, Embodied-RAG's memory is structured as a semantic forest, storing language descriptions at varying levels of detail. This hierarchical organization allows the system to efficiently generate context-sensitive outputs across different robotic platforms. We demonstrate that Embodied-RAG effectively bridges RAG to the robotics domain, successfully handling over 200 explanation and navigation queries across 19 environments, highlighting its promise for general-purpose nonparametric system for embodied agents.

      <br><hr>

      <center><h1>Task</h1></center>

      <table align="center" width="300px">
      <tr><td align="center" width="300px">
          <center><img src="./resources/sif_fig1_high_res.png" height="400px"></center>
      </td></tr>
      </table>
      
      <!-- Replacing white text with your provided content -->
      <div style="text-align: justify;">
          <p>The Embodied-RAG benchmark contains queries from the cross-product of <strong>{explicit, implicit, global}</strong> questions with potential <strong>{navigational action, language}</strong> generation outputs.</p>
      
          <h3>A task consists of:</h3>
          <ul>
              <li><strong>Query</strong>: The content can be explicit (e.g., a particular object instance), implicit (e.g., adequacy, instructions requiring pragmatic understanding), or global. The request may pertain to a specific location or overall ambiance.</li>
              <li><strong>Experience</strong>: The experience is a sequence of egocentric visual perception and odometry, occurring in indoor, outdoor, or mixed environments.</li>
              <li><strong>Output</strong>: The expected output may include both navigation actions with language descriptions (Fig. 1 top) or language explanations (Fig. 1 bottom).</li>
          </ul>
      
          <p>Example tasks are shown in Fig. 1, with instances of explicit, implicit, and global queries. Spatially, the queries range from specific regions small enough to contain certain objects to global regions encompassing the entire scene. Linguistically, global queries are closer to retrieval-augmented generation tasks, while explicit/implicit ones are more retrieval-focused. Explicit and implicit queries are navigational tasks that expect navigation actions and text descriptions of the retrieved location. Global queries, on the other hand, are explanation tasks requiring text generation at a more holistic level. There are no global navigation tasks, as they pertain to larger areas, sometimes the entire environment.</p>
      
          <p>Queries were collected by four human annotators using teleoperated robots across two real outdoor/mixed environments, three real indoor environments, and fourteen simulated environments. These environments include diverse settings such as a residential neighborhood, a deserted theme park, and a college campus. The environments convey different atmospheres through various activities—people lined up, chatting, or jackets left in office spaces. Navigation was performed by quadrupeds, drones, and locobots.</p>
      </div>
      
      <br><hr>

      <center><h1>Method</h1></center>

      <table align=center width=300px>
      <tr><td align=center width=300px>

          <center><img src = "./resources/sif_method.png" height="400px"></img></a><br></center>
      </td></tr>
      </table>

      Above, we show Reasoner, a baseline we implemented by combining popular SOTA methods such as FILM, LLM-Planner, and ReAct. It operates through three main components: (1) a semantic mapper that updates an allocentric map from egocentric RGB and depth inputs, (2) a prompt generator that represents prompts, and (3) the planner (GPT-4o) that generates high-level actions. The semantic mapper is called every timestep, whereas the prompt generator and planner are activated upon completion of the last high-level action or when a new decision is required. We additionally implement Prompter, an existing SOTA model for ALFRED.

      <br><hr>

      <center><h1>Results and Challenges</h1></center>

      <table align=center width=300px>
      <tr><td align=center width=300px>
          <center><img src = "./resources/sif_results.png" height="250px"></img></a><br></center>
      </td></tr>
      </table>

      We show SPL performance of Reasoner and Prompter on SIF tasks. This table shows that S_obj and S_hum tasks are challenging to both models. In a more detailed analysis, we find that both Reasoner and Prompter struggle in navigating through ambiguity in instructions. 

      <br><hr>


      <!-- <center><h1>Video Presentation</h1></center>

      <table align=center width=1000px>
      <tr><td align=center width=1000px>
          <iframe width="800" height="450" src="https://www.youtube.com/embed/iYQqetnCkJM" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
      </td></tr>
      </table>
      <br>
 -->
      

      <!-- <table align=center width=1000px>
        <p style="margin-top:4px;"></p>
        <tr><td width=1200px>
          <center><img src = "./resources/Teaser8.png" width="800px"></img><br></center>
        </td></tr>
      </table>
      <table align=center width=1000px>
        <p style="margin-top:4px;"></p>
        <tr><td width=1200px>
          <center><img src = "./resources/overview6.png" width="800px"></img><br></center>
        </td></tr>
      </table>
      <br/><hr> -->

      <!-- <center><h1>Short Presentation</h1></center>
      <div style="width:800px; margin:0 auto; text-align:justify">
        A short presentation at the CVPR 2020 Embodied AI Workshop describing our winning entry to the Habitat ObjectNav Challenge.
      </div><br/>
      <table align=center width=300px>
      <tr><td align=center width=300px>
        <iframe width="800" height="450" src="https://www.youtube.com/embed/4pk_5_NEW_s" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
      </td></tr>
      </table>
      <br><hr>

      <center><h1>Demo Video</h1></center>
      <table align=center width=300px>
      <tr><td align=center width=300px>
        <iframe width="640" height="360" src="https://www.youtube.com/embed/h56dA2uxpGU" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
      </td></tr>
      </table>
      <br><hr>

      <center><h1>Real-World Transfer</h1></center>
      <table align=center width=1000px>
        <p style="margin-top:4px;"></p>
        <tr><td width=1200px>
          <center><img src = "resources/semexp_real_world.gif" width="640px"></img><br></center>
        </td></tr>
      </table>
      <br/><hr>

	<center><h1>Media</h1></center>
	<div style="width:800px; margin:0 auto; text-align:justify">
        A short video created by CMU Media team describing the key ideas behind this project.   
     	 </div>
      <table align=center width=300px>
      <tr><td align=center width=300px>
        <iframe width="800" height="450" src="https://www.youtube.com/embed/FhIut4bqFyw" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
      </td></tr>
      </table>

      <br/>
 	
	<div style="width:800px; margin:0 auto; text-align:justify">
        Read more about the project in the following media articles:   
     	 </div>
	<table align=center width=700px>
	          <tr>
            <td align=left width=120px><span style="font-size:28px"><a onclick="window.open(this.href); return false;" href="https://techcrunch.com/2020/07/20/cmu-and-facebook-ai-research-use-machine-learning-to-teach-robots-to-navigate-by-recognizing-objects/" onkeypress="window.open(this.href); return false;">
									<img src="logos/tc.png" class="link_preview" 	style="max-width:130px;">
									</a></span></td>
            <td align=center width=120px><center><span style="font-size:28px"><a onclick="window.open(this.href); return false;" href="https://gizmodo.com/teaching-robots-there-are-no-toilets-in-the-kitchen-mak-1844446396" onkeypress="window.open(this.href); return false;">
									<img src="logos/gizmodo.png" class="link_preview" style="max-width:150px;">
									</a></span></center></td>
            <td align=center width=120px><center><span style="font-size:28px"><a onclick="window.open(this.href); return false;" href="https://www.thehindu.com/sci-tech/technology/a-navigation-system-powered-by-machine-learning-is-training-robots-to-recognise-objects/article32218732.ece" onkeypress="window.open(this.href); return false;">
									<img src="logos/hindu.png" class="link_preview" style="max-width:150px;">
									</a></span></center></td>
	    <td align=center width=120px><center><span style="font-size:28px"><a onclick="window.open(this.href); return false;" href="https://www.cmu.edu/news/stories/archives/2020/july/robot-navigation.html" onkeypress="window.open(this.href); return false;">
									<img src="logos/cmu.png" class="link_preview" style="max-width:150px;">
						</a></span></center></td>
          <tr/>
      </table>
      <br/><hr> -->

      




      <table align=center width=850px>
        <center><h1>Paper and Bibtex</h1></center>
        <tr>
        <td width=200px align=left>
        <!-- <p style="margin-top:4px;"></p> -->
        <a href="https://arxiv.org/abs/2212.05923"><img style="width:200px" src="resources/thumbnail_SIF.png"/></a>
        <center>
        <span style="font-size:20pt"><a href="https://arxiv.org/abs/2407.12061">[Paper]</a>
<!--        <span style="font-size:20pt"><a href="https://arxiv.org/abs/1902.05546v2">[ArXiv]</a>-->
<!--        <span style="font-size:20pt"><a href="resources/slides.pdf">[Slides]</a></span>-->
<!--        <span style="font-size:20pt"><a href="resources/poster.pdf">[Poster]</a></span>-->
        </center>
        </td>
        <td width=50px align=center>
        </td>
        <td width=550px align=left>
        <!-- <p style="margin-top:4px;"></p> -->
<!--            Chaplot, D.S., Gandhi, D., Gupta, S., Gupta, A. and Salakhutdinov, R., 2020. Learning To Explore Using Active Neural SLAM. In International Conference on Learning Representations (ICLR).-->
        <p style="text-align:left;"><b><span style="font-size:20pt">Citation</span></b><br/><span style="font-size:6px;">&nbsp;<br/></span> <span style="font-size:15pt">Min, S., Puig, X., Chaplot, D., Yang, T., Rai, A., Parashar, P., Salakhutdino,  R., Bisk, Y., Mottaghi, R. (2024). <br>
          Embodied-RAG: General Nonparametric Embodied
          Memory for Retrieval and Generation.
            <!-- <br> <em> ArXiv, abs/2212.05923.</em> </span></p> -->
        <!-- <p style="margin-top:20px;"></p> -->
        <!-- <span style="font-size:20pt"><a shape="rect" href="javascript:togglebib('assemblies19_bib')" class="togglebib">[Bibtex]</a></span> -->
        <div class="paper" id="assemblies19_bib">
                <pre xml:space="preserve">
<!-- Bibtex coming soon! -->
<!-- @misc{min2024situated,
      title={Situated Instruction Following}, 
      author={So Yeon Min and Xavi Puig and Ruslan Salakhutdinov and Yonatan Bisk and Roozbeh Mottaghi},
      year={2024},
      eprint={2214.05923},
      archivePrefix={arXiv},
      primaryClass={cs.RO}
} -->
@inproceedings{min2024situatedinstructionfollowing,
      title={Situated Instruction Following},
      author={So Yeon Min and Xavi Puig and Devendra Singh Chaplot and Tsung-Yen Yang 
      and Akshara Rai and Priyam Parashar and Ruslan Salakhutdinov and Yonatan Bisk 
      and Roozbeh Mottaghi},
      booktitle={ECCV},
      year={2024},
}           </pre>
          </div>
        </td>
        </tr>
        <tr>
        <td width=250px align=left>
        </td>
        <td width=50px align=center>
        </td>
        <td width=550px align=left>
          
          </td>
          </tr>
      </table>
    <br><hr>
  <br><br>
<script xml:space="preserve" language="JavaScript">
hideallbibs();
</script>
</body>
</html>

